{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the few models that are interpretable\n",
    "* High accuracy, stability and ease of interpretation\n",
    "* Allow for mapping of non-linear relationships\n",
    "* Adaptable at solving both classification and regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY WORDS: decision trees, random forest, gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithm (with predefined target variable), mostly used in classification problems. It works for both categorical and continuous input and output variables. The population or sample is split into sub-populations based on the most significant splitter/differentiatior in inut variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROOT NODE: represents the entire sample, which gets further divided into two or more subsets.\n",
    "* SPLITTING: process of dividing a node into two or more subnotes.\n",
    "* DECISION NODE: when a subnode splits into further sub-nodes\n",
    "* TERMINAL NODE / LEAF: a node which does not split any further\n",
    "* PRUNING: removal of subnodes (opposite of splitting)\n",
    "* BRANCH / SUBTREE: subsection of entire treee\n",
    "* PARENT AND CHILD NODES: node that is divided and nodes they get divided into to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](dt_terminology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easy to understand: even for people from non technical background; it does not need any statistical knowledge to read and interpret them. Intuitive graphical representation, easy to relate to initial hypothesis.\n",
    "* Useful in data expliration: decision trees are one of the fastest way to identify most significant variables and the relation among them. This method can help to create new features with more power to predict target variable. It can also help to identify most significant variables in the data exploration stage\n",
    "* Less data cleaning required: not influenced by outliers and missing values to a fair degree\n",
    "* No data type constraints: can handle both numerical and categorical variables\n",
    "* Non-parametric method: decision trees have no assumptions about the space distribution and the classifier structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Disavantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overfitting: to solve this problem set constraints on model parameteres and apply pruning\n",
    "* Not fit for continuous variables: while working with continuous variables, decision trees loose information when they categorize them into different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION TREE vs CLASSIFICATION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical.\n",
    "* In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value.\n",
    "* In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value.\n",
    "* Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes.\n",
    "* Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.\n",
    "* This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the the algorithm to stop once the number of observations per node becomes less than 50.\n",
    "* In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting. We’ll learn more about it in following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](types_of_decision_trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING OF TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different\n",
    "for classification and regression trees. \n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on type of target variables. There are four most commonly used algorithms in decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) GINI index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pure population, all items belong to the same class.\n",
    "\n",
    "* Categorical target variable (success or failure)\n",
    "* Splits are only binary\n",
    "* Higher Gini value means higher homogeneity\n",
    "* Used in CART (classification and regression trees) to create binary splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to calculate Gini for a split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Chi Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to find the statistical significance between the differences between subnodes and the parent node. It's measured by the sum of squares of the standardized diferences between observed and expected frequencies of the target variable.\n",
    "\n",
    "* Categorical target variable (success or failue)\n",
    "* Splits can be binary or more\n",
    "* Higher Chi-Square means higher significance of differences between subnodes and parent node\n",
    "* Genererates a tree called CHAID (chi-square automatic interaction detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Reduction in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY PARAMETERS OF TREE MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODELS vs LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "# for classification, here you can change the algorithm as gini or entropy (information gain) \n",
    "# by default it is gini  \n",
    "regressor = tree.DecisionTreeRegressor(criterion='mse', splitter='best', random_state=1) \n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "scores = cross_val_score(regressor, X, y, cv=10)\n",
    "print('Avg score: {}'.format(np.mean(scores)))\n",
    "ax = sns.tsplot(sorted(y))\n",
    "ax.errorbar(range(0, len(y)), sorted(y), yerr=np.nanmean(scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the best way to evaluate a regression tree?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Ever since the late, great Leo Breiman in the 90s (author of the original book about CART), the statistical industry has enshrined optimizing predictive accuracy (PA, alternatively, minimizing the mean squared error) as the gold standard metric for performance evaluation and model validation. Minimizing the mean squared error) as the gold standard metric for performance evaluation and model validation.\n",
    "\n",
    "That PA has evolved to this status is understandable: it’s easily calibrated and for the most part is a consistent statistic for internal model validation. However, all too often it is the sole criterion for model value, without thought given to wider business impact, and despite its being prone to p-hacking, gaming, and analyst fraud.\n",
    "\n",
    "My real point is that PA is not the only metric for model validation. Should the analyst be willing to sacrifice some PA by factoring in other considerations? Which combination of possible metrics will satisfy constraints wrt PA and derive the strongest strategic insights and confidence in the predictions in the face of truly out-of-sample information and the uncertainty inherent in all future projections?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Since you used cross-validation, MSE is the error you want to look at because it's the error from your testing set, and approximation of the true testing error.\n",
    "\n",
    "If you use other algorithms with cross-validation, MSE is still appropriate.\n",
    "\n",
    "You would only want to use an adjustment to MSE--like AIC, BIC, adj. R^2--if you only ran your models on the training set without any sort of validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tree (gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "** max_depth : int or None, optional (default=None) **\n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "The depth of a decision tree is the length of the longest path from a root to a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the maximum depth of a decision tree?**\n",
    "\n",
    "**A**: The absolute maximum depth would be N−1, where N is the number of training samples. You can derive this by considering that the least effective split would be peeling off one training example per node.\n",
    "\n",
    "In practice, no sane algorithm would reach this point:\n",
    "\n",
    "Most decision tree algorithms I've seen have multiple stopping criteria, including a user-defined depth and a minimum number of data points that it's willing to split on. For example, gbm's algorithm won't split nodes with 10 or fewer observations by default.\n",
    "\n",
    "If the tree uses any reasonable splitting criterion, it will almost always split off more than one observation at a time. That cuts down on the maximum depth pretty dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 98.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "# for classification, here you can change the algorithm as gini or entropy (information gain) \n",
    "# by default it is gini  \n",
    "model = tree.DecisionTreeClassifier(criterion='gini', splitter='best', random_state=1) \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n",
    "#Predict Output\n",
    "y_pred = model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "for i in range(1, len(y_test)):\n",
    "    res = 'correct' if y_test[i-1]==y_pred[i-1] else 'wrong'\n",
    "    # print(y_test[i-1], y_pred[i-1], res)\n",
    "\n",
    "print('Classification score: {}%'.format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split between train and test is fixed, by defining the random state.  if you use random_state=some_number, then you can guarantee that the output of Run 1 will be equal to the output of Run 2, i.e. your split will be always the same. It doesn't matter what the actual random_state number is 42, 0, 21, ... The important thing is that everytime you use 42, you will always get the same output the first time you make the split. This is useful if you want reproducible results, for example in the documentation, so that everybody can consistently see the same numbers when they run the examples. In practice I would say, you should set the random_state to some fixed number while you test stuff, but then remove it in production if you really need a random (and not a fixed) split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q: If the variance is not coming from the training set, then why is the result not always the same? Is there variability inroduced in the decision tree classifier? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The DecisionTreeClassifier works by repeatedly splitting the training data, based on the value of some feature. The Scikit-learn implementation lets you choose between a few splitting algorithms by providing a value to the splitter keyword argument.\n",
    "\n",
    "\"best\" randomly chooses a feature and finds the 'best' possible split for it, according to some criterion (which you can also choose; see the methods signature and the criterion argument). It looks like the code does this N_feature times, so it's actually quite like a bootstrap.\n",
    "\n",
    "\"random\" chooses the feature to consider at random, as above. However, it also then tests randomly-generated thresholds on that feature (random, subject to the constraint that it's between its minimum and maximum values). This may help avoid 'quantization' errors on the tree where the threshold is strongly influenced by the exact values in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "\n",
    "tree.export_graphviz(model,\n",
    "                    out_file='gini_tree.dot',\n",
    "                    feature_names=iris.feature_names,\n",
    "                    class_names=iris.target_names,\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    impurity=False)\n",
    "\n",
    "!rm gini_tree.png\n",
    "!dot -Tpng gini_tree.dot -o gini_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gini_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification tree: information gain ('entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 98.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "# for classification, here you can change the algorithm as gini or entropy (information gain) \n",
    "# by default it is gini  \n",
    "model = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', random_state=1,\n",
    "                                    max_depth=None,\n",
    "                                    min_samples_leaf=10) \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n",
    "#Predict Output\n",
    "y_pred = model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "for i in range(1, len(y_test)):\n",
    "    res = 'correct' if y_test[i-1]==y_pred[i-1] else 'wrong'\n",
    "    # print(y_test[i-1], y_pred[i-1], res)\n",
    "\n",
    "print('Classification score: {}%'.format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "\n",
    "tree.export_graphviz(model,\n",
    "                    out_file='entropy_tree.dot',\n",
    "                    feature_names=iris.feature_names,\n",
    "                    class_names=iris.target_names,\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    impurity=False)\n",
    "\n",
    "!rm entropy_tree.png\n",
    "!dot -Tpng entropy_tree.dot -o entropy_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a browser caching issue, the image is not reloaded if changed on disk after first load. To solve this, the current time stamp is added to the file name to force the browser to reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"entropy_tree.png?1523213092.022874\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg = time.time()\n",
    "img = \"entropy_tree.png?{}\".format(arg)\n",
    "Image(url= img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE METHODS IN TREE BASED MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods group predictive models in order to achieve better accuracy and model stability. Ensembre modeels are known to concede considerable boosts to tree based models.\n",
    "\n",
    "As any other method, tree based models also suffer from the plague of bias and variance:\n",
    "\n",
    "**Bias:** how much do the predicted values differ from the actual value in average\n",
    "\n",
    "**Variance:** how much will the predicitons vary if different samples are taken from the same population\n",
    "\n",
    "Normally as you increase the complexity of your model, the prediction error will decrease due to lower bias. As you keep making your model more complex, you start over-fitting your model, and it will start suffering from high variance. The model should maintain a balance between these two types of erros: the so called **bias-variance trade-off** management of errors. Ensemble learning is one way to execute this trade-off.\n",
    "\n",
    "![title](model_complexity.png)\n",
    "\n",
    "Some of the commonly used ensemble methods include:\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique used to reduce the variance of predictions by combining the result of multiple classifiers, trained on different sub-samples of the same dataset.\n",
    "\n",
    "**1) Create multiple datasets:** \n",
    "* sampling is done *with* replacement on the original dataset, forming new smaller datasets\n",
    "* the new datasets can be subsets both in terms of samples as in term of features (corresponding fractions can be set as hyper parameters in the bagging model)\n",
    "* Row and column (sample and feature) fractions smaller than 1 help in making more robust models, less prone to overfitting\n",
    "\n",
    "**2) Build multiple classifiers:**\n",
    "* Classifiers are built on top of each dataset\n",
    "* Generally the same classifier is modeled on each of the datasets, and the predictions are made\n",
    "\n",
    "**3) Combine classifiers:**\n",
    "* The predictions of all the classifiers are combined using the mean, median or mode value, depending on the problem at hand\n",
    "* The combined values are generally more robust than each a single model (each classifier but itself)\n",
    "\n",
    "\n",
    "The number of models built is *NOT* a hyper-parameter. Higher number of models are always better or may give similar performance than lower numbers. Theoretically provable, the variance of the combined predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM vs XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
